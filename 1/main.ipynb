{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b20d0e3",
   "metadata": {},
   "source": [
    "Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a609efe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1000,  0.2000,  3.1212],\n",
      "        [ 9.0000,  2.3000, 21.0000],\n",
      "        [ 1.2000, 32.1000,  2.0000]])\n",
      "Data type: torch.float32\n",
      "tensor([[ 0.1000,  0.2000,  3.1212],\n",
      "        [ 9.0000,  2.3000, 21.0000],\n",
      "        [ 1.2000, 32.1000,  2.0000]], dtype=torch.float64)\n",
      "Data type: torch.float64\n"
     ]
    }
   ],
   "source": [
    "# tensor ma matrix hoy ne enu data type same hoy of all the data \n",
    "# that is included in the tensor, it is a matrix which is multi-dimensional\n",
    "# tensor is created from a list\n",
    "import torch\n",
    "lis = [[0.1, 0.2, 3.121221], [9, 2.3, 21], [1.2, 32.1, 2]]\n",
    "tens = torch.tensor(lis)\n",
    "print(tens)\n",
    "\n",
    "print(\"Data type:\", tens.dtype) \n",
    "# even if i increase the number of decimal places, still it shows float32\n",
    "\n",
    "tens2 = torch.tensor(lis, dtype=torch.float64)\n",
    "print(tens2)\n",
    "print(\"Data type:\", tens2.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85e67183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before change: tensor(0.2000)\n",
      "After change: tensor(9.9999)\n",
      "tensor([0.1000, 9.9999, 3.1212])\n"
     ]
    }
   ],
   "source": [
    "# change karva mate normal aj che\n",
    "print(\"Before change:\", tens[0][1])\n",
    "\n",
    "tens[0][1] = 9.9999\n",
    "print(\"After change:\", tens[0][1])\n",
    "\n",
    "# another way to access\n",
    "print(tens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5e58b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient at x=5.2 for h is: tensor(562.4319)\n",
      "Gradient at x=5.2 for z is: tensor(643.5519)\n"
     ]
    }
   ],
   "source": [
    "# now auto_grad is the thing which helps in backpropagation automatically\n",
    "x = torch.tensor(5.2, requires_grad=True)\n",
    "y = x ** 3\n",
    "z = y * 1 + 2 \n",
    "h = x ** 4\n",
    "h.backward()\n",
    "print(\"Gradient at x=5.2 for h is:\", x.grad)  \n",
    "z.backward()\n",
    "print(\"Gradient at x=5.2 for z is:\", x.grad) \n",
    "\n",
    "# i got to know that the gradients can be calculated only for scalar values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42f31b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addition: tensor([ 5,  7, 36])\n",
      "Subtraction: tensor([-3, -3, 24])\n",
      "Multiplication: tensor([  4,  10, 180])\n",
      "Division: tensor([0.2500, 0.4000, 5.0000])\n",
      "Matrix Addition:\n",
      " tensor([[10, 10, 10],\n",
      "        [10, 10, 10],\n",
      "        [10, 10, 10]])\n",
      "Matrix Subtraction:\n",
      " tensor([[-8, -6, -4],\n",
      "        [-2,  0,  2],\n",
      "        [ 4,  6,  8]])\n",
      "Matrix Multiplication:\n",
      " tensor([[ 9, 16, 21],\n",
      "        [24, 25, 24],\n",
      "        [21, 16,  9]])\n",
      "Matrix Division:\n",
      " tensor([[0.1111, 0.2500, 0.4286],\n",
      "        [0.6667, 1.0000, 1.5000],\n",
      "        [2.3333, 4.0000, 9.0000]])\n"
     ]
    }
   ],
   "source": [
    "# arithmetic is simple\n",
    "ten1 = [1,2,30]\n",
    "ten2 = [4,5,6]\n",
    "\n",
    "tensor_ten1 = torch.tensor(ten1)\n",
    "tensor_ten2 = torch.tensor(ten2)\n",
    "print(\"Addition:\", tensor_ten1 + tensor_ten2)\n",
    "print(\"Subtraction:\", tensor_ten1 - tensor_ten2)\n",
    "print(\"Multiplication:\", tensor_ten1 * tensor_ten2)\n",
    "print(\"Division:\", tensor_ten1 / tensor_ten2)\n",
    "\n",
    "wew = [[1,2,3],[4,5,6],[7,8,9]]\n",
    "wew2 = [[9,8,7],[6,5,4],[3,2,1]]\n",
    "tensor_wew1 = torch.tensor(wew)\n",
    "tensor_wew2 = torch.tensor(wew2)\n",
    "print(\"Matrix Addition:\\n\", tensor_wew1 + tensor_wew2)\n",
    "print(\"Matrix Subtraction:\\n\", tensor_wew1 - tensor_wew2)   \n",
    "print(\"Matrix Multiplication:\\n\", tensor_wew1 * tensor_wew2)\n",
    "print(\"Matrix Division:\\n\", tensor_wew1 / tensor_wew2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b1c1923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PowBackward0 object at 0x000001FF0F1173D0>\n"
     ]
    }
   ],
   "source": [
    "# Only floating-point tensors can require gradients\n",
    "# it builds a dag during forward pass\n",
    "x1 = torch.tensor(2.0, requires_grad=True)\n",
    "y1 = x1 ** 2\n",
    "\n",
    "print(y1.grad_fn) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f26b2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient at x=3.0 is: tensor(6.)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "y = x ** 2\n",
    "z = y + 5\n",
    "z.backward()\n",
    "print(\"Gradient at x=3.0 is:\", x.grad)\n",
    "print(y.retain_grad())\n",
    "\n",
    "# in place operations break gradient tracking\n",
    "# autograd only stores gradients for leaf tensors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c4be47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz/dx: tensor(6.)\n",
      "dz/dy: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "y = x ** 2\n",
    "y.retain_grad()\n",
    "z = y + 5\n",
    "\n",
    "z.backward()\n",
    "\n",
    "print(\"dz/dx:\", x.grad)\n",
    "print(\"dz/dy:\", y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72343b1",
   "metadata": {},
   "source": [
    "Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da36314e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v + v: tf.Tensor([2. 4. 6.], shape=(3,), dtype=float32)\n",
      "M * scalar:\n",
      " tf.Tensor(\n",
      "[[2. 4.]\n",
      " [6. 8.]], shape=(2, 2), dtype=float32)\n",
      "element ops:\n",
      " tf.Tensor(\n",
      "[[ 1.  4.]\n",
      " [ 9. 16.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# tf ma constant and Variable lakhvu pade, if variable hoy to change karay like weights\n",
    "a = tf.constant(3.0)\n",
    "v = tf.constant([1.0, 2.0, 3.0])\n",
    "M = tf.constant([[1.0, 2.0],\n",
    "                 [3.0, 4.0]])\n",
    "\n",
    "print(\"v + v:\", v + v)\n",
    "print(\"M * scalar:\\n\", M * 2)\n",
    "print(\"element ops:\\n\", tf.square(M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9568fb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A x B:\n",
      " tf.Tensor(\n",
      "[[22. 28.]\n",
      " [49. 64.]], shape=(2, 2), dtype=float32)\n",
      "Transpose of A:\n",
      " tf.Tensor(\n",
      "[[1. 4.]\n",
      " [2. 5.]\n",
      " [3. 6.]], shape=(3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "A = tf.constant([[1.0, 2.0, 3.0],\n",
    "                 [4.0, 5.0, 6.0]])\n",
    "\n",
    "B = tf.constant([[1.0, 2.0],\n",
    "                 [3.0, 4.0],\n",
    "                 [5.0, 6.0]])\n",
    "\n",
    "C = tf.matmul(A, B) # main yaad rakhvanu e ke not like pytorch, tensorflow ma matrix multiplication mate alag function che\n",
    "# it can't be done by simply A * B\n",
    "A_T = tf.transpose(A)\n",
    "\n",
    "print(\"A x B:\\n\", C)\n",
    "print(\"Transpose of A:\\n\", A_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8784c7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution of Ax=b:\n",
      " [[-0.49999988]\n",
      " [ 0.99999994]]\n",
      "Eigenvalues: [2.763932  7.2360687]\n",
      "Eigenvectors:\n",
      " [[ 0.85065085  0.52573115]\n",
      " [-0.52573115  0.85065085]]\n",
      "L2 norm: 10.246951\n"
     ]
    }
   ],
   "source": [
    "M = tf.constant([[4.0, 7.0],\n",
    "                 [2.0, 6.0]])\n",
    "\n",
    "b = tf.constant([[5.0],\n",
    "                 [5.0]])\n",
    "\n",
    "# solve kare Ax = b\n",
    "x = tf.linalg.solve(M, b)\n",
    "# symmetric matrix ma real value result kare\n",
    "eigen_vals, eigen_vecs = tf.linalg.eigh(M)\n",
    " \n",
    "l2_norm = tf.norm(M, ord=2)\n",
    "\n",
    "# shape ne dtype evu badhu nai print karava mate\n",
    "print(\"Solution of Ax=b:\\n\", x.numpy())\n",
    "print(\"Eigenvalues:\", eigen_vals.numpy())\n",
    "print(\"Eigenvectors:\\n\", eigen_vecs.numpy())\n",
    "print(\"L2 norm:\", l2_norm.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290f469a",
   "metadata": {},
   "source": [
    "Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21492542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND Gate\n",
      "[0.0, 0.0] -> 0.0 (prob: 0.004485994577407837 )\n",
      "[0.0, 1.0] -> 0.0 (prob: 0.1331961452960968 )\n",
      "[1.0, 0.0] -> 0.0 (prob: 0.1331855207681656 )\n",
      "[1.0, 1.0] -> 1.0 (prob: 0.8397309184074402 )\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "X = torch.tensor([\n",
    "    [0., 0.],\n",
    "    [0., 1.],\n",
    "    [1., 0.],\n",
    "    [1., 1.]\n",
    "])\n",
    "\n",
    "y_and = torch.tensor([0., 0., 0., 1.])\n",
    "\n",
    "w = torch.randn(2, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "\n",
    "lr = 0.1\n",
    "\n",
    "for epoch in range(1000):\n",
    "    loss = 0\n",
    "    # iterable tuple banave che\n",
    "    for x, y in zip(X, y_and):\n",
    "        z = torch.dot(w, x) + b\n",
    "        y_hat = torch.sigmoid(z)\n",
    "        loss += (y_hat - y) ** 2\n",
    "\n",
    "    # w.grad and b.grad now contain ∂loss/∂w and ∂loss/∂b\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        w -= lr * w.grad\n",
    "        b -= lr * b.grad\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "\n",
    "threshold = 0.5\n",
    "\n",
    "print(\"AND Gate\")\n",
    "for x in X:\n",
    "    prob = torch.sigmoid(torch.dot(w, x) + b)\n",
    "    out = 1.0 if prob >= threshold else 0.0\n",
    "    print(x.tolist(), \"->\", out, \"(prob:\", float(prob), \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2caf6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OR Gate\n",
      "[0.0, 0.0] -> 0.0 (prob: 0.1321832835674286 )\n",
      "[0.0, 1.0] -> 1.0 (prob: 0.9195904731750488 )\n",
      "[1.0, 0.0] -> 1.0 (prob: 0.918760359287262 )\n",
      "[1.0, 1.0] -> 1.0 (prob: 0.9988237023353577 )\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "X = torch.tensor([\n",
    "    [0., 0.],\n",
    "    [0., 1.],\n",
    "    [1., 0.],\n",
    "    [1., 1.]\n",
    "])\n",
    "\n",
    "y_or = torch.tensor([0., 1., 1., 1.])\n",
    "\n",
    "w = torch.randn(2, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "\n",
    "lr = 0.1\n",
    "\n",
    "for epoch in range(1000):\n",
    "    loss = 0\n",
    "    for x, y in zip(X, y_or):\n",
    "        z = torch.dot(w, x) + b\n",
    "        y_hat = torch.sigmoid(z)\n",
    "        loss += (y_hat - y) ** 2\n",
    "\n",
    "    # w.grad and b.grad now contain ∂loss/∂w and ∂loss/∂b\n",
    "    loss.backward()\n",
    "\n",
    "    # prevents autograd from tracking updates\n",
    "    with torch.no_grad():\n",
    "        w -= lr * w.grad\n",
    "        b -= lr * b.grad\n",
    "        w.grad.zero_() # stop from accumulating gradients\n",
    "        b.grad.zero_()\n",
    "\n",
    "threshold = 0.5\n",
    "\n",
    "print(\"OR Gate\")\n",
    "for x in X:\n",
    "    prob = torch.sigmoid(torch.dot(w, x) + b)\n",
    "    out = 1.0 if prob >= threshold else 0.0\n",
    "    print(x.tolist(), \"->\", out, \"(prob:\", float(prob), \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96284854",
   "metadata": {},
   "source": [
    "Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5a8cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1000/10000], Loss: 0.0002\n",
      "Epoch [2000/10000], Loss: 0.0001\n",
      "Epoch [3000/10000], Loss: 0.0000\n",
      "Epoch [4000/10000], Loss: 0.0000\n",
      "Epoch [5000/10000], Loss: 0.0000\n",
      "Epoch [6000/10000], Loss: 0.0000\n",
      "Epoch [7000/10000], Loss: 0.0000\n",
      "Epoch [8000/10000], Loss: 0.0000\n",
      "Epoch [9000/10000], Loss: 0.0000\n",
      "Epoch [10000/10000], Loss: 0.0000\n",
      "\n",
      "Predictions:\n",
      "Input: [0.0, 0.0], Output: 0.0\n",
      "Input: [0.0, 1.0], Output: 1.0\n",
      "Input: [1.0, 0.0], Output: 1.0\n",
      "Input: [1.0, 1.0], Output: 0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "X = torch.tensor([[0., 0.], [0., 1.], [1., 0.], [1., 1.]], dtype=torch.float32)\n",
    "y = torch.tensor([[0.], [1.], [1.], [0.]], dtype=torch.float32)\n",
    "\n",
    "# every trainable model must inherit from nn.Module as ena thi .parameters() and .train() jeva functions available thay\n",
    "class XOR(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XOR, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 2) # input to hidden layer\n",
    "        self.fc2 = nn.Linear(2, 1) # hidden to output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x)) # hidden layer activation\n",
    "        x = torch.sigmoid(self.fc2(x)) # output activation\n",
    "        return x\n",
    "\n",
    "model = XOR()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.02)\n",
    "\n",
    "num_epochs = 10000\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step() # performs parameter update\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "model.eval() # switches to evaluation mode\n",
    "# dropout and batchnorm behave different\n",
    "with torch.no_grad():\n",
    "    predictions = model(X)\n",
    "    rounded_predictions = (predictions >= 0.5).float()\n",
    "    print(\"\\nPredictions:\")\n",
    "    for i in range(len(X)):\n",
    "        print(f\"Input: {X[i].tolist()}, Output: {rounded_predictions[i].item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9de0700",
   "metadata": {},
   "source": [
    "Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8ec868fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\anaconda3\\envs\\ai-gpu\\lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([6, 1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted value: 2.878230094909668\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor([\n",
    "    [0., 0.],\n",
    "    [0., 1.],\n",
    "    [1., 0.],\n",
    "    [1., 1.],\n",
    "    [2., 3.],\n",
    "    [4., 5.]\n",
    "])\n",
    "\n",
    "y = torch.tensor([\n",
    "    [0.],\n",
    "    [1.],\n",
    "    [1.],\n",
    "    [2.],\n",
    "    [5.],\n",
    "    [9.]\n",
    "])\n",
    "\n",
    "class NETWORK(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NETWORK, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 2)\n",
    "        self.fc2 = nn.Linear(2, 1) \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)   # raw value\n",
    "       \n",
    "model = NETWORK()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.02)\n",
    "\n",
    "for epoch in range(10000):\n",
    "    outputs = model(x) # forward pass\n",
    "    loss = criterion(outputs, y) # calculate loss\n",
    "    optimizer.zero_grad() # zero the gradients\n",
    "    loss.backward() # backpropagation\n",
    "    optimizer.step() # update parameters\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    user_x = input(\"Enter 2 numbers separated by space: \")\n",
    "    user_x = torch.tensor([list(map(float, user_x.split()))])\n",
    "    prediction = model(user_x)\n",
    "    print(\"Predicted value:\", prediction.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
